This is the description for HW2: https://github.com/schneider128k/machine_learning_course/blob/master/homework/hw2.md

I created five different architectures and use K-Fold validation to reevaluate the best model.

Overall, several key points can be concluded:
    * Deep network is better, but it requires much more training time.
    * Drop Out layer is a solution for over-fitting problem
    * Adam optimizer is better than SGD
    * Data Augmentation has significant improvement for model's performance on validation dataset
    * K-folder validation tends to have a better performance than the simple hold-out validation. 
